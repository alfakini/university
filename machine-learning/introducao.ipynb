{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introdução\n",
    "\n",
    "Baseado no trabalho do professor [Thiago Santos](https://github.com/thsant/sklearn-intro).\n",
    "\n",
    "Há um conjunto de técnicas de **aprendizado** e **inferência** muito grande. Alguns exemplos:\n",
    "\n",
    "* Regressão Linear\n",
    "* Regressão Logística\n",
    "* Clustering (diversos algoritmos)\n",
    "* Máquinas de suporte vetorial\n",
    "* Processos Gaussianos\n",
    "* PCA/LDA\n",
    "\n",
    "Tais técnicas apresentam nomes diferentes em comunidades diferentes:\n",
    "\n",
    "* Cientistas da Computação e engenheiros chamam de **aprendizado de máquina**\n",
    "* Economistas chamam de **econometria**\n",
    "* Estatísticos de **aprendizado estatístico de padrões**\n",
    "* Bioinformatas de **bioestatística**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Qual é a tarefa em questão?\n",
    "\n",
    "Essas técnicas tentam resolver um **problema de aprendizado**: dadas **n amostras** de dados, **predizer** as propriedades de dados não observados.\n",
    "\n",
    "Problemas de aprendizado são geralmente dividos em 2 casos:\n",
    "\n",
    "**Aprendizado supervisionado** - no qual as amostras apresentam informação extra sobre a propriedade a ser prevista.\n",
    "\n",
    "* Classificação: a propriedade a ser prevista é uma classe/categoria\n",
    "    * *Exemplo*: classificar *e-mail* como SPAM/HAM a partir de seu conteúdo\n",
    "\n",
    "* Regressão: a propriedade a ser prevista é um valor contínuo\n",
    "    * *Exemplo*: o *preço* de uma casa a partir de dados de localização, cômodos, vizinhança, etc.\n",
    "\n",
    "**Aprendizado não-supervisionado** - no qual as amostras não apresentam informação extra, ou seja, desejamos buscar alguma **estrutura** nesses dados (aglomerar dados semelhantes e/ou determinar a distribuição de probabilidade que gerou tais dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visão geral da Sklearn\n",
    "\n",
    "[Visão geral](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "![ML map](http://scikit-learn.org/stable/_static/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, load_boston\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=1000, centers=20, random_state=123)\n",
    "labels = [\"b\", \"r\"]\n",
    "y = np.take(labels, (y < 10))\n",
    "print(X) \n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a 2 dimensional array, with 1000 rows and 2 columns\n",
    "print(X.shape)\n",
    " \n",
    "# y is a vector of 1000 elements\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows and columns can be accessed with lists, slices or masks\n",
    "print(X[[1, 2, 3]])     # rows 1, 2 and 3\n",
    "print(X[:5])            # 5 first rows\n",
    "print(X[500:510, 0])    # values from row 500 to row 510 at column 0\n",
    "print(X[y == \"b\"][:5])  # 5 first rows for which y is \"b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure()\n",
    "for label in labels:\n",
    "    mask = (y == label)\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=label)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(boston.data[:,5], boston.target)\n",
    "plt.xlabel(u'RM (número médio de cômodos)')\n",
    "plt.ylabel(u'Valor médio (em US$ 1.000)')\n",
    "plt.title('Boston House Prices dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API única\n",
    "\n",
    "Todos os algoritmos de aprendizado do scikit-learn compartilham uma API uniforme e limitada que consiste em interfaces complementares:\n",
    "\n",
    "* uma interface **estimator** para modelos de construção e montagem;\n",
    "* uma interface **predictor** para fazer previsões;\n",
    "* uma interface **transformer** para converter dados.\n",
    "\n",
    "O objetivo é aplicar uma API simples e consistente para tornar trivial trocar ou conectar algoritmos.\n",
    "\n",
    "## Fit/Predict\n",
    "\n",
    "- No Sklearn, os algoritmos são representados por objetos (POO)\n",
    "- Tais objetos implementam a **interface fit/predict**\n",
    "- `fit`\n",
    "    - realiza a etapa de **aprendizado**\n",
    "- `predict`\n",
    "    - realiza as etapas de **regressão** ou **classificação**\n",
    "- O **modelo** aprendido pode ser armazenado em disco utilizando o módulo de persistência `pickle`\n",
    "\n",
    "### Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de estimador\n",
    "class Estimator(object):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits estimator to data.\"\"\"\n",
    "        # set state of ``self``\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the nearest neighbor class\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Change this to try \n",
    "                                                    # something else\n",
    "\n",
    "# Set hyper-parameters, for controlling algorithm\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Learn a model from training data\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions  \n",
    "print(clf.predict(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute (approximate) class probabilities\n",
    "print(clf.predict_proba(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial import plot_surface    \n",
    "plot_surface(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial import plot_histogram    \n",
    "plot_histogram(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier zoo\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "Árvores de decisão são modelos estatísticos que utilizam um treinamento supervisionado para a classificação e previsão de dados. Em outras palavras, em sua construção é utilizado um conjunto de treinamento formado por entradas e saídas. Estas últimas são as classes.\n",
    "\n",
    "![Arvore](https://web.tecnico.ulisboa.pt/ana.freitas/bioinformatics.ath.cx/bioinformatics.ath.cx/uploads/RTEmagicC_arv_dec3.gif.gif)\n",
    "\n",
    "[Documentação](http://scikit-learn.org/stable/modules/tree.html)\n",
    "[Tutorial](http://www.r2d3.us/uma-introducao-visual-ao-aprendizado-de-maquina-1/)\n",
    "\n",
    "Por exemplo, no exemplo abaixo, as árvores de decisão aprendem com dados para aproximar uma curva de seno com um conjunto de regras de decisão if-then-else. Quanto mais profunda for a árvore, mais complexa será a decisão e mais ajustada ao modelo.\n",
    "\n",
    "![](https://raw.githubusercontent.com/data-science-joinville/notebooks/b8c3bc2a6268cd003956abd6bd61a841a003a42a/notebooks/titanic/docs/decision-tree-examplee.png)\n",
    "<center>[Referência](https://sebastianraschka.com/faq/docs/bagging-boosting-rf.html)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial import plot_clf\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Construir várias árvores de decisão e tirar a média das suas decisões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "clf = RandomForestClassifier(n_estimators=500)\n",
    "# from sklearn.ensemble import ExtraTreesClassifier \n",
    "# clf = ExtraTreesClassifier(n_estimators=500)\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "[Documentação](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "[Tutorial](http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/)\n",
    "[Logistic Function](https://www.wikiwand.com/en/Logistic_function)\n",
    "\n",
    "No regressor logístico tenta-se prever a probabilidade de um dado exemplo pertencer a classe de sobreviventes versos a probabilidade de percenter a classe de falecidos.\n",
    "\n",
    "$f(x) = \\frac{L}{1 + \\mathrm e^{-k(x-x_0)}}$\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/data-science-joinville/notebooks/b8c3bc2a6268cd003956abd6bd61a841a003a42a/notebooks/titanic/docs/logistic-curve.png)\n",
    "Para a curva padrão\n",
    "$L=1,k=1,x_0=0$\n",
    "\n",
    "#### Um exemplo\n",
    "\n",
    "![](https://raw.githubusercontent.com/data-science-joinville/notebooks/b8c3bc2a6268cd003956abd6bd61a841a003a42a/notebooks/titanic/docs/logistic-regression-example.jpg)\n",
    "\n",
    "[Referência](https://www.mssqltips.com/sqlservertip/3471/introduction-to-the-sql-server-analysis-services-logistic-regression-data-mining-algorithm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "[Documentação](http://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    "Usa um subconjunto de pontos de treinamento na função de decisão (chamados vetores de suporte), portanto é eficiente na memória.\n",
    "\n",
    "![](https://raw.githubusercontent.com/data-science-joinville/notebooks/b8c3bc2a6268cd003956abd6bd61a841a003a42a/notebooks/titanic/docs/svm-example.jpg)\n",
    "[Referência](http://www-personal.umich.edu/~johnhugo/commercial/inc/img/SVM.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\")  # try kernel=\"rbf\" instead\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "\n",
    "É uma rede neural semelhante à perceptron, mas com mais de uma camada de neurônios em alimentação direta. Tal tipo de rede é composta por camadas de neurônios ligadas entre si por sinapses com pesos. O aprendizado nesse tipo de rede é geralmente feito através do algoritmo de retro-propagação do erro. \n",
    "\n",
    "![](https://camo.githubusercontent.com/d95fb90b396fc77c614cc6b176dd049066273f96/68747470733a2f2f7777772e64726f70626f782e636f6d2f732f717334746f6a763575356834386c662f6d756c74696c617965725f70657263657074726f6e2e706e673f7261773d31)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation=\"relu\", learning_rate=\"invscaling\")\n",
    "clf.fit(X, y)\n",
    "plot_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aprendizado supervisionado no Sklearn\n",
    "\n",
    "- Nearest Neighbors\n",
    "- Support Vector Machines (SVM)\n",
    "    - Linear Support\n",
    "    - Radial Basis Function (RGB) kernel SVM\n",
    "- Decision Trees\n",
    "- Ensemble\n",
    "    - Random Forests\n",
    "    - AdaBoost\n",
    "- Linear Discriminant Analysis\n",
    "- Gaussian Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "def classifier_showroom(names, classifiers):\n",
    "    h = .02  # step size in the mesh\n",
    "\n",
    "    X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "    rng = np.random.RandomState(2)\n",
    "    X += 2 * rng.uniform(size=X.shape)\n",
    "    linearly_separable = (X, y)\n",
    "\n",
    "    datasets = [make_moons(noise=0.3, random_state=0),\n",
    "                make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "                linearly_separable\n",
    "                ]\n",
    "\n",
    "    figure = plt.figure(figsize=(12, 8))\n",
    "    i = 1\n",
    "    # iterate over datasets\n",
    "    for ds in datasets:\n",
    "        # preprocess dataset, split into training and test part\n",
    "        X, y = ds\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "\n",
    "        # just plot the dataset first\n",
    "        cm = plt.cm.RdBu\n",
    "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        i += 1\n",
    "\n",
    "        # iterate over classifiers\n",
    "        for name, clf in zip(names, classifiers):\n",
    "            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "\n",
    "            # Plot the decision boundary. For that, we will assign a color to each\n",
    "            # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "            if hasattr(clf, \"decision_function\"):\n",
    "                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "            else:\n",
    "                Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "            # Put the result into a color plot\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "            # Plot also the training points\n",
    "            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "            # and testing points\n",
    "            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                       alpha=0.6)\n",
    "\n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "            ax.set_title(name)\n",
    "            ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                    size=15, horizontalalignment='right')\n",
    "            i += 1\n",
    "\n",
    "    figure.subplots_adjust(left=.02, right=.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1)]\n",
    "\n",
    "classifier_showroom(names, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "names = [\"Decision Tree\", \"Random Forest\", \"AdaBoost\"]\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier()]\n",
    "classifier_showroom(names, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "names = [\"Naive Bayes\", \"LDA\", \"QDA\"]\n",
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    LDA(),\n",
    "    QDA()]\n",
    "\n",
    "classifier_showroom(names, classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Aprendizado não-supervisionado no Sklearn\n",
    "\n",
    "- Gaussian mixture models\n",
    "- Clustering\n",
    "    - Affinity propagation\n",
    "    - Mean-shift\n",
    "    - Spectral clustering\n",
    "    - Hierarchical clustering\n",
    "    - DBSCAN\n",
    "- Neural Networks (unsupervised)\n",
    "    - Restricted Boltzmann machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def clustering_showroom():\n",
    "\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from sklearn import cluster, datasets\n",
    "    from sklearn.metrics import euclidean_distances\n",
    "    from sklearn.neighbors import kneighbors_graph\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Generate datasets. We choose the size big enough to see the scalability\n",
    "    # of the algorithms, but not too big to avoid too long running times\n",
    "    n_samples = 1500\n",
    "    noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                          noise=.05)\n",
    "    noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "    blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "    no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "    colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "    colors = np.hstack([colors] * 20)\n",
    "\n",
    "    plt.figure(figsize=(17, 9.5))\n",
    "    plt.subplots_adjust(left=.001, right=.999, bottom=.001, top=.96, wspace=.05,\n",
    "                        hspace=.01)\n",
    "\n",
    "    plot_num = 1\n",
    "    for i_dataset, dataset in enumerate([noisy_circles, noisy_moons, blobs,\n",
    "                                         no_structure]):\n",
    "        X, y = dataset\n",
    "        # normalize dataset for easier parameter selection\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "        # estimate bandwidth for mean shift\n",
    "        bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)\n",
    "\n",
    "        # connectivity matrix for structured Ward\n",
    "        connectivity = kneighbors_graph(X, n_neighbors=10)\n",
    "        # make connectivity symmetric\n",
    "        connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "        # Compute distances\n",
    "        #distances = np.exp(-euclidean_distances(X))\n",
    "        distances = euclidean_distances(X)\n",
    "\n",
    "        # create clustering estimators\n",
    "        ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "        two_means = cluster.MiniBatchKMeans(n_clusters=2)\n",
    "        ward = cluster.AgglomerativeClustering(n_clusters=2,\n",
    "                        linkage='ward', connectivity=connectivity)\n",
    "        spectral = cluster.SpectralClustering(n_clusters=2,\n",
    "                                              eigen_solver='arpack',\n",
    "                                              affinity=\"nearest_neighbors\")\n",
    "        dbscan = cluster.DBSCAN(eps=.2)\n",
    "        affinity_propagation = cluster.AffinityPropagation(damping=.9,\n",
    "                                                           preference=-200)\n",
    "\n",
    "        average_linkage = cluster.AgglomerativeClustering(linkage=\"average\",\n",
    "                                affinity=\"cityblock\", n_clusters=2,\n",
    "                                connectivity=connectivity)\n",
    "\n",
    "        for name, algorithm in [\n",
    "                                ('MiniBatchKMeans', two_means),\n",
    "                                ('AffinityPropagation', affinity_propagation),\n",
    "                                ('MeanShift', ms),\n",
    "                                ('SpectralClustering', spectral),\n",
    "                                ('Ward', ward),\n",
    "                                ('AgglomerativeClustering', average_linkage),\n",
    "                                ('DBSCAN', dbscan)\n",
    "                               ]:\n",
    "            # predict cluster memberships\n",
    "            t0 = time.time()\n",
    "            algorithm.fit(X)\n",
    "            t1 = time.time()\n",
    "            if hasattr(algorithm, 'labels_'):\n",
    "                y_pred = algorithm.labels_.astype(np.int)\n",
    "            else:\n",
    "                y_pred = algorithm.predict(X)\n",
    "\n",
    "            # plot\n",
    "            plt.subplot(4, 7, plot_num)\n",
    "            if i_dataset == 0:\n",
    "                plt.title(name, size=18)\n",
    "            plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n",
    "\n",
    "            if hasattr(algorithm, 'cluster_centers_'):\n",
    "                centers = algorithm.cluster_centers_\n",
    "                center_colors = colors[:len(centers)]\n",
    "                plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
    "            plt.xlim(-2, 2)\n",
    "            plt.ylim(-2, 2)\n",
    "            plt.xticks(())\n",
    "            plt.yticks(())\n",
    "            plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                     transform=plt.gca().transAxes, size=15,\n",
    "                     horizontalalignment='right')\n",
    "            plot_num += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "clustering_showroom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instalação da Sklearn\n",
    "\n",
    "- Usuários Windows e Mac deveriam considerar a [distribuição Anaconda](https://store.continuum.io/cshop/anaconda/) da Continuum Analytics\n",
    "\n",
    "- Usuários de todos os sistemas podem utilizar o `pip`\n",
    "    $ pip install scikit-learn\n",
    "\n",
    "- [Instruções detalhadas de instalação](http://scikit-learn.org/stable/install.html) podem ser vistas no site oficial do sklearn."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
